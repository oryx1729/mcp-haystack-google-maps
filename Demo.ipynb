{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb75db5-09bb-49bf-82e3-f7de79e4c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "# Allows nested asyncio event loops - Jupyter already runs in an event loop,\n",
    "# so nest_asyncio lets us use asyncio.run() within this environment\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b359b0d-0770-404e-b224-91a373db1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.tools import ToolInvoker\n",
    "from haystack.components.converters import OutputAdapter\n",
    "from haystack.dataclasses import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d974e73-bc01-409c-9803-6b81532b10ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_integrations.tools.mcp.mcp_tool import SSEServerInfo\n",
    "from mcp_server import MCPServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ac19f5d-24bb-4c87-b705-e70a0affaf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e16d2701-cacd-4c02-97ba-e7a19b6e0e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in sse_reader: peer closed connection without sending complete message body (incomplete chunked read)\n",
      "Error in sse_reader: peer closed connection without sending complete message body (incomplete chunked read)\n",
      "Error in sse_reader: peer closed connection without sending complete message body (incomplete chunked read)\n",
      "Error in sse_reader: peer closed connection without sending complete message body (incomplete chunked read)\n",
      "Error in sse_reader: peer closed connection without sending complete message body (incomplete chunked read)\n",
      "Error in sse_reader: peer closed connection without sending complete message body (incomplete chunked read)\n",
      "Error in sse_reader: peer closed connection without sending complete message body (incomplete chunked read)\n"
     ]
    }
   ],
   "source": [
    "google_maps_mcp = MCPServer(server_info=SSEServerInfo(base_url=f\"http://localhost:8000\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9780910-6e95-42c6-bfad-7ae4753977bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsafe mode is enabled. This allows execution of arbitrary code in the Jinja template. Use this only if you trust the source of the template.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x11220ba30>\n",
       "🚅 Components\n",
       "  - llm: OpenAIChatGenerator\n",
       "  - tool_invoker: ToolInvoker\n",
       "  - adapter: OutputAdapter\n",
       "  - response_llm: OpenAIChatGenerator\n",
       "🛤️ Connections\n",
       "  - llm.replies -> tool_invoker.messages (List[ChatMessage])\n",
       "  - llm.replies -> adapter.initial_tool_messages (List[ChatMessage])\n",
       "  - tool_invoker.tool_messages -> adapter.tool_messages (List[ChatMessage])\n",
       "  - adapter.output -> response_llm.messages (list[ChatMessage])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.add_component(\"llm\", OpenAIChatGenerator(model=\"gpt-4o\", tools=google_maps_mcp))\n",
    "pipeline.add_component(\"tool_invoker\", ToolInvoker(tools=google_maps_mcp))\n",
    "pipeline.add_component(\n",
    "    \"adapter\",\n",
    "    OutputAdapter(\n",
    "        template=\"{{ initial_msg + initial_tool_messages + tool_messages }}\",\n",
    "        output_type=list[ChatMessage],\n",
    "        unsafe=True,\n",
    "    ),\n",
    ")\n",
    "pipeline.add_component(\"response_llm\", OpenAIChatGenerator(model=\"gpt-4o-mini\"))\n",
    "\n",
    "\n",
    "pipeline.connect(\"llm.replies\", \"tool_invoker.messages\")\n",
    "pipeline.connect(\"llm.replies\", \"adapter.initial_tool_messages\")\n",
    "pipeline.connect(\"tool_invoker.tool_messages\", \"adapter.tool_messages\")\n",
    "pipeline.connect(\"adapter.output\", \"response_llm.messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "125573ac-08d7-4792-9458-25d114c703dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pipeline(user_input):\n",
    "    user_input_msg = ChatMessage.from_user(text=user_input)\n",
    "    result = pipeline.run({\n",
    "        \"llm\": {\"messages\": [user_input_msg]}, \n",
    "        \"adapter\": {\"initial_msg\": [user_input_msg]}\n",
    "    }, include_outputs_from={\"tool_invoker\"})\n",
    "    \n",
    "    print(\"\\nUser:\", user_input)\n",
    "    print(\"\\nAssistant:\", result[\"response_llm\"][\"replies\"][0].text)\n",
    "    print(\"\\nTool invocations:\")\n",
    "    for tool_call in result[\"tool_invoker\"].get(\"tool_calls\", []):\n",
    "        print(f\"  - Tool: {tool_call.get('name')}\")\n",
    "        print(f\"    Arguments: {tool_call.get('arguments')}\")\n",
    "        print(f\"    Response: {tool_call.get('response')}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e9e4420-de69-4be9-8617-c439961f5240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: Which city has higher elevation, Frankurt or Paris?\n",
      "\n",
      "Assistant: To compare the elevations:\n",
      "\n",
      "- **Frankfurt, Germany**: The city is situated at an elevation of about 112 meters (367 feet) above sea level.\n",
      "- **Paris, France**: The city has a lower elevation of around 35 meters (115 feet) above sea level.\n",
      "\n",
      "Therefore, **Frankfurt** has a higher elevation than **Paris**.\n",
      "\n",
      "Tool invocations:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tool_invoker': {'tool_messages': [ChatMessage(_role=<ChatRole.TOOL: 'tool'>, _content=[ToolCallResult(result='meta=None content=[TextContent(type=\\'text\\', text=\\'{\\\\n  \"places\": [\\\\n    {\\\\n      \"name\": \"Frankfurt am Main\",\\\\n      \"formatted_address\": \"Frankfurt am Main, Germany\",\\\\n      \"location\": {\\\\n        \"lat\": 50.1109221,\\\\n        \"lng\": 8.6821267\\\\n      },\\\\n      \"place_id\": \"ChIJxZZwR28JvUcRAMawKVBDIgQ\",\\\\n      \"types\": [\\\\n        \"locality\",\\\\n        \"political\"\\\\n      ]\\\\n    }\\\\n  ]\\\\n}\\', annotations=None)] isError=False', origin=ToolCall(tool_name='maps_search_places', arguments={'query': 'Frankfurt, Germany'}, id='call_zyvP8iGaVcGUatNQtwd3ZyKB'), error=False)], _name=None, _meta={}),\n",
       "   ChatMessage(_role=<ChatRole.TOOL: 'tool'>, _content=[ToolCallResult(result='meta=None content=[TextContent(type=\\'text\\', text=\\'{\\\\n  \"places\": [\\\\n    {\\\\n      \"name\": \"Paris\",\\\\n      \"formatted_address\": \"Paris, France\",\\\\n      \"location\": {\\\\n        \"lat\": 48.8575475,\\\\n        \"lng\": 2.3513765\\\\n      },\\\\n      \"place_id\": \"ChIJD7fiBh9u5kcRYJSMaMOCCwQ\",\\\\n      \"types\": [\\\\n        \"locality\",\\\\n        \"political\"\\\\n      ]\\\\n    }\\\\n  ]\\\\n}\\', annotations=None)] isError=False', origin=ToolCall(tool_name='maps_search_places', arguments={'query': 'Paris, France'}, id='call_ymVemP1MtBbotwZAjdwPmgUQ'), error=False)], _name=None, _meta={})],\n",
       "  'state': <haystack.dataclasses.state.State at 0x111ce48e0>},\n",
       " 'response_llm': {'replies': [ChatMessage(_role=<ChatRole.ASSISTANT: 'assistant'>, _content=[TextContent(text='To compare the elevations:\\n\\n- **Frankfurt, Germany**: The city is situated at an elevation of about 112 meters (367 feet) above sea level.\\n- **Paris, France**: The city has a lower elevation of around 35 meters (115 feet) above sea level.\\n\\nTherefore, **Frankfurt** has a higher elevation than **Paris**.')], _name=None, _meta={'model': 'gpt-4o-mini-2024-07-18', 'index': 0, 'finish_reason': 'stop', 'usage': {'completion_tokens': 75, 'prompt_tokens': 385, 'total_tokens': 460, 'completion_tokens_details': CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), 'prompt_tokens_details': PromptTokensDetails(audio_tokens=0, cached_tokens=0)}})]}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_pipeline(user_input = \"Which city has higher elevation, Frankurt or Paris?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f2593-4937-430c-b89c-7b3877af1ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
